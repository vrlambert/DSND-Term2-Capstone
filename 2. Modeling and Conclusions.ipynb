{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Sparkify Project - Part 2\n\nIf you didn't read the first part, you should! This one picks up where it leaves off and starts using the full 12GB dataset. We will first verify our assumptions from the first notebook, then run the feature engineering script and train a few basic models to see how they perform. First we'll create the spark session and import some libraries."}, {"metadata": {"scrolled": false, "trusted": true}, "cell_type": "code", "source": "# Starter code\nfrom pyspark.sql import SparkSession", "execution_count": 1, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ce9d7eb980c34d76b2d6f5f1aacb9c46"}}, "metadata": {}}, {"output_type": "stream", "text": "Starting Spark application\n", "name": "stdout"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>application_1549211192169_0004</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-47-73.us-east-2.compute.internal:20888/proxy/application_1549211192169_0004/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-38-194.us-east-2.compute.internal:8042/node/containerlogs/container_1549211192169_0004_01_000001/livy\">Link</a></td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "stream", "text": "SparkSession available as 'spark'.\n", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "import pyspark.sql.functions as sf\nimport pyspark.sql.types as st\n\nfrom pyspark.ml.feature import VectorAssembler\n\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.classification import LogisticRegression\n\nfrom pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, GBTClassifier", "execution_count": 2, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c1a629f9c9634a96b12be19e3eaa82f3"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Create spark session\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Sparkify\") \\\n    .getOrCreate()", "execution_count": 3, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9dd9a26532ff48ffb9eaee5211f92249"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Next read in the full dataset from the public s3 bucket!"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# Read in full sparkify dataset\nevent_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\ndf_raw = spark.read.json(event_data)", "execution_count": 4, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "c4dbda86ec5b4126ac80e56d06869a39"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Next check the head and the schema to see if everything looks similar:"}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "df_raw.head()", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f9aca238b844428f8df0d049cc6dfb14"}}, "metadata": {}}, {"output_type": "stream", "text": "Row(artist=u'Popol Vuh', auth=u'Logged In', firstName=u'Shlok', gender=u'M', itemInSession=278, lastName=u'Johnson', length=524.32934, level=u'paid', location=u'Dallas-Fort Worth-Arlington, TX', method=u'PUT', page=u'NextSong', registration=1533734541000, sessionId=22683, song=u'Ich mache einen Spiegel - Dream Part 4', status=200, ts=1538352001000, userAgent=u'\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId=u'1749042')", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_raw.printSchema()", "execution_count": 12, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "root\n |-- artist: string (nullable = true)\n |-- auth: string (nullable = true)\n |-- firstName: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- itemInSession: long (nullable = true)\n |-- lastName: string (nullable = true)\n |-- length: double (nullable = true)\n |-- level: string (nullable = true)\n |-- location: string (nullable = true)\n |-- method: string (nullable = true)\n |-- page: string (nullable = true)\n |-- registration: long (nullable = true)\n |-- sessionId: long (nullable = true)\n |-- song: string (nullable = true)\n |-- status: long (nullable = true)\n |-- ts: long (nullable = true)\n |-- userAgent: string (nullable = true)\n |-- userId: string (nullable = true)"}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Exploration"}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "df_raw.count()", "execution_count": 9, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "79befdccc63b482ea4730c77856270d8", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "26259199"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Holy cow, that's a big increase from the subset. Although we expected it."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_raw.select([sf.count(sf.when(sf.isnull(c), c)).alias(c) for c in df_raw.columns]).show()", "execution_count": 16, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c4e929c88ce545d9b214dd23ec5f848b", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "+-------+----+---------+------+-------------+--------+-------+-----+--------+------+----+------------+---------+-------+------+---+---------+------+\n| artist|auth|firstName|gender|itemInSession|lastName| length|level|location|method|page|registration|sessionId|   song|status| ts|userAgent|userId|\n+-------+----+---------+------+-------------+--------+-------+-----+--------+------+----+------------+---------+-------+------+---+---------+------+\n|5408927|   0|   778479|778479|            0|  778479|5408927|    0|  778479|     0|   0|      778479|        0|5408927|     0|  0|   778479|     0|\n+-------+----+---------+------+-------------+--------+-------+-----+--------+------+----+------------+---------+-------+------+---+---------+------+"}]}, {"metadata": {}, "cell_type": "markdown", "source": "After looking at some missing values, it looks like the pattern is the same as the subset. Let's just check for the `\"\"` user like before."}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "df_raw.filter(df_raw.userId == \"\").count()", "execution_count": 10, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "40a84ed33deb45d0892cf1d733cfc654", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "0"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Hmmmm, that's weird, there aren't any. Let's see what user shows up then?"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_raw.where(sf.isnull(df_raw.registration)).select('userId').distinct().show()", "execution_count": 17, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "+-------+\n| userId|\n+-------+\n|1261737|\n+-------+"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "df_raw.where(sf.isnull(df_raw.gender)).select('userId').distinct().show()", "execution_count": 18, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "d0a1f7713b6a4f55bebfc7a045b72a73", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "+-------+\n| userId|\n+-------+\n|1261737|\n+-------+"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Wow, looks like `1261737` is the null user instead of empty string!"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.filter(df.userId==1261737).count()", "execution_count": 21, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "778479"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df_raw.where(sf.isnull(df_raw.registration)).select('sessionId').distinct().show()", "execution_count": 20, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "dd43674bc8d54c0ca4b54aec264a3597", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "+---------+\n|sessionId|\n+---------+\n|    23116|\n|    33760|\n|    30428|\n|    35323|\n|    35484|\n|    13248|\n|    38878|\n|    38543|\n|    27919|\n|    48763|\n|    52001|\n|    17048|\n|    52051|\n|    48899|\n|    47492|\n|    49983|\n|    52611|\n|    50049|\n|    50329|\n|    50287|\n+---------+\nonly showing top 20 rows"}]}, {"metadata": {}, "cell_type": "markdown", "source": "And it looks like sessions aren't specific problem."}, {"metadata": {}, "cell_type": "markdown", "source": "## Cleaning\n\nWe will use the same functions from the first notebook."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def convert_ms(x):\n    \"\"\"Converts given ns to ms\"\"\"\n    if x is None:\n        return None\n    \n    return x//1000\n\nconvert_ms_udf = sf.udf(convert_ms, st.LongType())", "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "92148258180c4c898efb078540f9766c"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def clean_df(df_raw):\n    \"\"\"\n    Takes in a raw events dataframe, makes a few extra columns and cleans it.\n    \"\"\"\n    df = df_raw.filter(df_raw.userId != 1261737)\n    \n    df = df.withColumn('timestamp', convert_ms_udf(df.ts).cast('timestamp'))\n    df = df.withColumn('registration_ts', convert_ms_udf(df.registration).cast('timestamp'))\n    \n    return df", "execution_count": 6, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9ea42da028254ab99b29d2a86c6d6c14"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "# after reading in the df just running this cell catches up with the exploration\ndf = clean_df(df_raw)", "execution_count": 7, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f4edb20e01524368bc7e98ce1399118d"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.select([sf.count(sf.when(sf.isnull(c), c)).alias(c) for c in df_raw.columns]).show()", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "9ab12e8d66e845f6bdc476d3c4b0cc83"}}, "metadata": {}}, {"output_type": "stream", "text": "+-------+----+---------+------+-------------+--------+-------+-----+--------+------+----+------------+---------+-------+------+---+---------+------+\n| artist|auth|firstName|gender|itemInSession|lastName| length|level|location|method|page|registration|sessionId|   song|status| ts|userAgent|userId|\n+-------+----+---------+------+-------------+--------+-------+-----+--------+------+----+------------+---------+-------+------+---+---------+------+\n|4630448|   0|        0|     0|            0|       0|4630448|    0|       0|     0|   0|           0|        0|4630448|     0|  0|        0|     0|\n+-------+----+---------+------+-------------+--------+-------+-----+--------+------+----+------------+---------+-------+------+---+---------+------+", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we can check out the number of users and sessions in the full data, turns out there are still a sizeable number."}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "df.agg(sf.countDistinct('userId'), sf.countDistinct('sessionId')).show()", "execution_count": 25, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "d0a5b74a9e5c48aca2a4cd67a05b7919", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "+----------------------+-------------------------+\n|count(DISTINCT userId)|count(DISTINCT sessionId)|\n+----------------------+-------------------------+\n|                 22277|                   223096|\n+----------------------+-------------------------+"}]}, {"metadata": {}, "cell_type": "markdown", "source": "It also turns out the timestamp range is similar to the other data, although the registration range is longer as we would expect from a larger user group."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.agg(sf.min('timestamp'), sf.max('timestamp')).show()", "execution_count": 30, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "d421b090c44f43a2950c960a5f6f6aea", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "+-------------------+-------------------+\n|     min(timestamp)|     max(timestamp)|\n+-------------------+-------------------+\n|2018-10-01 00:00:01|2018-12-01 00:00:02|\n+-------------------+-------------------+"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "df.agg(sf.min('registration_ts'), sf.max('registration_ts')).show()", "execution_count": 29, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "4a30ec498e694b439eb207c99ed7bd82", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "+--------------------+--------------------+\n|min(registration_ts)|max(registration_ts)|\n+--------------------+--------------------+\n| 2017-10-14 22:05:25| 2018-12-03 07:23:42|\n+--------------------+--------------------+"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Interesting the that last registration is after the end of the events data - seems like a possible error but probably won't be a big deal."}, {"metadata": {}, "cell_type": "markdown", "source": "## Moving On"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def get_feature_df(df):\n    \"\"\"Takes in a cleaned event dataframe and returns a feature dataframe\"\"\"\n    \n    # Column names for the vector\n    vector_cols = []\n    \n    # Get session counts\n    session_counts = df.groupby('userId').agg(sf.countDistinct('sessionId').alias('session_count'))\n    vector_cols.append('session_count')\n    \n    # Get pages and ignore events\n    pages = df.select('page').distinct().sort('page')\n    pages_list = [r.page for r in pages.collect()]\n    drop_events = ['Cancel']\n    \n    # Get event counts\n    feat_df = df.groupby('userId').pivot('page', pages_list).count()\n    feat_df = feat_df.withColumnRenamed('Cancellation Confirmation', 'label')\n    feat_df = feat_df.drop(*drop_events).fillna(0)\n    \n    feat_df = feat_df.join(session_counts, on='userId')\n    \n    # Normalize by session counts\n    ignore_cols = {'userId', 'session_count', 'label'}\n    remaining_cols = sorted(list(set(feat_df.columns) - ignore_cols))\n    for column in remaining_cols:\n        feat_df = feat_df.withColumn(column, sf.col(column) / feat_df.session_count)\n    vector_cols.extend(remaining_cols)\n    \n    # Get account ages\n    max_timestamp = df.agg(sf.max('timestamp')).first()[0]\n    account_ages = df.select('userId', \n                             sf.datediff(sf.lit(max_timestamp), df.registration_ts).alias('account_age')).distinct()\n    \n    vector_cols.append('account_age')\n    feat_df = feat_df.join(account_ages, on='userId')\n    \n    # Get weekly song counts\n    week_counts = df.where(df.page=='NextSong') \\\n                .groupby('userId', sf.date_trunc('week', 'timestamp').cast('date').alias('week')).count() \\\n                .groupby('userId').pivot('week').sum().fillna(0)\n    \n    vector_cols.extend(week_counts.columns[1:])\n    feat_df = feat_df.join(week_counts, on='userId')\n    \n    # Get genders\n    genders = df.select('userId', sf.when(sf.col('gender')=='F', 0).otherwise(1).alias('genders')).distinct()\n    \n    vector_cols.append('genders')\n    feat_df = feat_df.join(genders, on='userId')\n    \n    # Assemble the vector\n    assembler = VectorAssembler(inputCols=vector_cols, outputCol='features')\n    \n    return assembler.transform(feat_df)", "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "311045b7acbf41d498764794690cf81e"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we set up the feature dataframe and make sure everything is ok."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "feature_df = get_feature_df(df)", "execution_count": null, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "feature_df.printSchema()", "execution_count": 10, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "d3d73939e749491c8bf1430e60a4446e"}}, "metadata": {}}, {"output_type": "stream", "text": "root\n |-- userId: string (nullable = true)\n |-- About: double (nullable = true)\n |-- Add Friend: double (nullable = true)\n |-- Add to Playlist: double (nullable = true)\n |-- label: long (nullable = true)\n |-- Downgrade: double (nullable = true)\n |-- Error: double (nullable = true)\n |-- Help: double (nullable = true)\n |-- Home: double (nullable = true)\n |-- Logout: double (nullable = true)\n |-- NextSong: double (nullable = true)\n |-- Roll Advert: double (nullable = true)\n |-- Save Settings: double (nullable = true)\n |-- Settings: double (nullable = true)\n |-- Submit Downgrade: double (nullable = true)\n |-- Submit Upgrade: double (nullable = true)\n |-- Thumbs Down: double (nullable = true)\n |-- Thumbs Up: double (nullable = true)\n |-- Upgrade: double (nullable = true)\n |-- session_count: long (nullable = false)\n |-- account_age: integer (nullable = true)\n |-- 2018-10-01: long (nullable = true)\n |-- 2018-10-08: long (nullable = true)\n |-- 2018-10-15: long (nullable = true)\n |-- 2018-10-22: long (nullable = true)\n |-- 2018-10-29: long (nullable = true)\n |-- 2018-11-05: long (nullable = true)\n |-- 2018-11-12: long (nullable = true)\n |-- 2018-11-19: long (nullable = true)\n |-- 2018-11-26: long (nullable = true)\n |-- genders: integer (nullable = false)\n |-- features: vector (nullable = true)", "name": "stdout"}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "feature_df.persist()", "execution_count": 11, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "24846212d69345fe9796aba97292a1bb"}}, "metadata": {}}, {"output_type": "stream", "text": "DataFrame[userId: string, About: double, Add Friend: double, Add to Playlist: double, label: bigint, Downgrade: double, Error: double, Help: double, Home: double, Logout: double, NextSong: double, Roll Advert: double, Save Settings: double, Settings: double, Submit Downgrade: double, Submit Upgrade: double, Thumbs Down: double, Thumbs Up: double, Upgrade: double, session_count: bigint, account_age: int, 2018-10-01: bigint, 2018-10-08: bigint, 2018-10-15: bigint, 2018-10-22: bigint, 2018-10-29: bigint, 2018-11-05: bigint, 2018-11-12: bigint, 2018-11-19: bigint, 2018-11-26: bigint, genders: int, features: vector]", "name": "stdout"}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "feature_df.head(1)", "execution_count": 12, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f10491eea2c14fe7837cb875e3a47068"}}, "metadata": {}}, {"output_type": "stream", "text": "[Row(userId=u'1000280', About=0.0, Add Friend=0.6363636363636364, Add to Playlist=1.1363636363636365, label=1, Downgrade=0.13636363636363635, Error=0.13636363636363635, Help=0.36363636363636365, Home=2.0, Logout=0.6818181818181818, NextSong=46.45454545454545, Roll Advert=3.3636363636363638, Save Settings=0.045454545454545456, Settings=0.4090909090909091, Submit Downgrade=0.045454545454545456, Submit Upgrade=0.045454545454545456, Thumbs Down=1.5, Thumbs Up=2.409090909090909, Upgrade=0.4090909090909091, session_count=22, account_age=95, 2018-10-01=308, 2018-10-08=34, 2018-10-15=291, 2018-10-22=57, 2018-10-29=187, 2018-11-05=38, 2018-11-12=107, 2018-11-19=0, 2018-11-26=0, genders=1, features=DenseVector([22.0, 0.0, 0.6364, 1.1364, 0.1364, 0.1364, 0.3636, 2.0, 0.6818, 46.4545, 3.3636, 0.0455, 0.4091, 0.0455, 0.0455, 1.5, 2.4091, 0.4091, 95.0, 308.0, 34.0, 291.0, 57.0, 187.0, 38.0, 107.0, 0.0, 0.0, 1.0]))]", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Next let's check the number of users in each class:"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "feature_df.groupby('label').count().show()", "execution_count": 15, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "c8779c9c6f7d456a845063da2c67f3cb", "version_major": 2, "version_minor": 0}, "text/plain": "VBox()"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "+-----+-----+\n|label|count|\n+-----+-----+\n|    0|17259|\n|    1| 5002|\n+-----+-----+"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Looks like a decent balance after all!"}, {"metadata": {}, "cell_type": "markdown", "source": "## Modeling\n\nFirst we'll split the data into a test and train sets, then we'll run through a few different models to see which is the most effective. My goal for this project is not to use a complicated algorithm to proceed, but rather to try out a few basic algorithms available in pyspark and see how they do. There are quite a few deicision tree varieties available so this seems like a good opportunity to compare and contrast them."}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "train, test = feature_df.randomSplit([0.8, 0.2], seed=42)", "execution_count": 13, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "cd1c7f1ce57f468fbfdc0d2f9e2553dd"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "We can define this useful function to print out some evaluation metrics from the results vector."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "def get_metrics(res):\n    \"\"\"Given a results vector returns accuracy and f1-score\"\"\"\n    total = res.count()\n    \n    tp = res.where((res.label==1) & (res.prediction==1)).count()\n    tn = res.where((res.label==0) & (res.prediction==0)).count()\n    \n    fp = res.where((res.label==0) & (res.prediction==1)).count()\n    fn = res.where((res.label==1) & (res.prediction==0)).count()\n        \n    accuracy = (1.0*tp + tn) / total\n    precision = 1.0*tp / (tp + fp)\n    recall = 1.0*tp / (tp + fn)\n    f1 = 2.0 * (precision * recall) / (precision + recall)\n    \n    print('Accuracy: ', round(accuracy, 2))\n    print('Precision: ', round(precision, 2))\n    print('Recall: ', round(recall, 2))\n    print('F1-Score: ', round(f1, 2))", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "4dd99c44c7614640b73d572c6ee27b33"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "I want to start with a logistic regression just to get a feel for some code, but then I'm going to move on to decision trees. Luckily, decision trees don't require feature scaling so it should be ok in the current state of the feature vector. First we will create the model."}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "lr =  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0)", "execution_count": 14, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "59c7a81d196a45d5a014f7c9f70c381d"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Then, train on the training dataset."}, {"metadata": {"scrolled": false, "trusted": true}, "cell_type": "code", "source": "lr_model = lr.fit(train)", "execution_count": 15, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f0fc911e014444218a9532e7f31abb4f"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Finally, generate the results vector from the test set."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "results_lr = lr_model.transform(test)", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1349e1235ea140a0b15047729c9993e4"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "get_metrics(results_lr)", "execution_count": 19, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3f8cb89b634e450bae6fc891349bb1f7"}}, "metadata": {}}, {"output_type": "stream", "text": "('Accuracy: ', 0.85)\n('Precision: ', 0.78)\n('Recall: ', 0.46)\n('F1-Score: ', 0.58)", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Looks like the logisitic regression model actually performed fairly well! Not great but not terribly. Next we can move on to a basic decision tree."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "dt = DecisionTreeClassifier()\ndt_model = dt.fit(train)\nresults_dt = dt_model.transform(test)\nget_metrics(results_dt)", "execution_count": 25, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ee2a52b2963d4e38973165fd58c18ba2"}}, "metadata": {}}, {"output_type": "stream", "text": "('Accuracy: ', 0.9)\n('Precision: ', 0.8)\n('Recall: ', 0.71)\n('F1-Score: ', 0.76)", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Well the decision tree actually performed quite well! 0.76 f1-score is respectable, and 0.9 accuracy is great. Next let's see how Random Forests does."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "rf = RandomForestClassifier()", "execution_count": 16, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "2c56e50fb4954c959f8cf472bcf89258"}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "rf_model = rf.fit(train)\nresults_rf = rf_model.transform(test)\nget_metrics(results_rf)", "execution_count": 27, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "3bc286fff0b044c3946a5580067c00e0"}}, "metadata": {}}, {"output_type": "stream", "text": "('Accuracy: ', 0.89)\n('Precision: ', 0.93)\n('Recall: ', 0.57)\n('F1-Score: ', 0.71)", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Hmmmm, it's weird that the random forest wouldn't perform as well as a basic decision tree. It does seem like there is potential for improvement over a plain decision tree because the precision is much higher here. Maybe some parameter tuning could help it improve, although I definitely found this surprising. This result actually was similar for gradient boosted trees which I decided to cut from the project in favor of random forests. Also model training and testing takes quite a long time on pyspark even with a relatively small dataset, those were both complications that showed up during this section.\n\nSo even though random forests had a lower f1-score out of the bat I will try tuning them to see if we can get higher performance than just the decision tree."}, {"metadata": {}, "cell_type": "markdown", "source": "## Refinement"}, {"metadata": {}, "cell_type": "markdown", "source": "It's really advantageous to use the `CrossValidator` here because it means we can try out a variety of parameters and automatically select the best ones using a validation set. First we will check out random forests and see if we can improve the performance to above what we had with the base decision tree. For parameters I chose the max depth of the tree, the min number of samples per node, and the number of trees:"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "rf_paramGrid = ParamGridBuilder() \\\n                .addGrid(rf.maxDepth, [5, 7]) \\\n                .addGrid(rf.minInstancesPerNode, [1, 3]) \\\n                .addGrid(rf.numTrees, [20, 40]) \\\n                .build()", "execution_count": 17, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Next we have to build the crossvalidator - binary classification evaluator uses area under ROC as a optimization metric which is pretty good in this case."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "rf_crossval = CrossValidator(estimator = rf,\n                             estimatorParamMaps=rf_paramGrid,\n                             evaluator = BinaryClassificationEvaluator(),\n                             numFolds=3)", "execution_count": 18, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "ed03558215f94f65a35f50cf226a98a9"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Next up, train the model and predict just as before."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "rf_cross_model = rf_crossval.fit(train)", "execution_count": 19, "outputs": []}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "results_rf_cross = rf_cross_model.transform(test)", "execution_count": 20, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "f99cf1e08c4840a5aea4cbd281d84d9c"}}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "An added bonus of using crossval is that we can peek at the average metrics of each sub-model to see how the overall performance varies. Looks like we do pretty well with all models being between 0.91 and 0.93 area under ROC."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "rf_cross_model.avgMetrics", "execution_count": 21, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "dc06b3f0e95e467287892e671fbf389b"}}, "metadata": {}}, {"output_type": "stream", "text": "[0.9169346891040968, 0.9173023225091668, 0.9254453792776469, 0.9257542682113922, 0.9173122897441244, 0.9169262158854977, 0.9253361009197547, 0.9255381851780136]", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we can check out the final metrics."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "get_metrics(results_rf_cross)", "execution_count": 22, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1dd19ad4dbd9453a9f8b4a0380810bcb"}}, "metadata": {}}, {"output_type": "stream", "text": "('Accuracy: ', 0.91)\n('Precision: ', 0.91)\n('Recall: ', 0.69)\n('F1-Score: ', 0.79)", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Wow it seems like it does perform better than the plain old decision tree, that's great! I wonder what the parameter set was:"}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "print('Num Trees', rf_cross_model.bestModel.getNumTrees)\nprint('Max Depth', rf_cross_model.bestModel._java_obj.getMaxDepth())\nprint('Min Samples Node', rf_cross_model.bestModel._java_obj.getMinInstancesPerNode())", "execution_count": 23, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "6c38fb3bf06f4e7bb8782e36c64ca28a"}}, "metadata": {}}, {"output_type": "stream", "text": "('Num Trees', 20)\n('Max Depth', 7)\n('Min Samples Node', 3)", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Interesting that it's only 20 trees, I expected 40 to have better performance. It seems like searching the tree deeper (7 laters) with lower granularity (3 samples per node instead of 1) gives the best performance with random forests. We can also look at the most important features too."}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "rf_importances = list(rf_cross_model.bestModel.featureImportances)", "execution_count": 30, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "b151b505116445f897541f4a648aefa5"}}, "metadata": {}}]}, {"metadata": {"scrolled": true, "trusted": true}, "cell_type": "code", "source": "cols = feature_df.columns[1:4]+feature_df.columns[5:-1]", "execution_count": 28, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"metadata": {"trusted": true}, "cell_type": "code", "source": "from pprint import pprint", "execution_count": 33, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "1d3f414a974c4e6883abd959deb268ac"}}, "metadata": {}}]}, {"metadata": {"scrolled": false, "trusted": true}, "cell_type": "code", "source": "pprint(sorted(zip(cols, rf_importances), key=lambda x: x[1], reverse=True))", "execution_count": 34, "outputs": [{"output_type": "display_data", "data": {"text/plain": "VBox()", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": "958e8ebc093a4bfc824c5b4bb75c88e8"}}, "metadata": {}}, {"output_type": "stream", "text": "[('2018-11-26', 0.24872878836907866),\n ('2018-11-19', 0.17520050382643623),\n ('2018-10-01', 0.1438442430274558),\n ('2018-11-12', 0.11298791939802035),\n ('2018-11-05', 0.08173734301357599),\n ('About', 0.05818187911482335),\n ('2018-10-29', 0.03639303510577493),\n ('2018-10-08', 0.03188444021589177),\n ('Save Settings', 0.01682809675748398),\n ('Error', 0.016804971016558194),\n ('Thumbs Up', 0.01662328184502604),\n ('2018-10-22', 0.016306980101822275),\n ('2018-10-15', 0.008291761522428883),\n ('Thumbs Down', 0.005079927902946381),\n ('Upgrade', 0.004928414662012405),\n ('Roll Advert', 0.003517665888227861),\n ('Submit Downgrade', 0.0035063281160905573),\n ('NextSong', 0.0027751695854539874),\n ('Logout', 0.002392298084843223),\n ('Add to Playlist', 0.00235308900223043),\n ('session_count', 0.002241110387822691),\n ('account_age', 0.0017137474992622945),\n ('Downgrade', 0.001537621796301837),\n ('Add Friend', 0.0015071196473681932),\n ('Home', 0.0014890784435418571),\n ('Settings', 0.0013405297050881988),\n ('Help', 0.000854574630188457),\n ('Submit Upgrade', 0.0007032759751303001),\n ('genders', 0.00024680535911511905)]", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Well it seems like the song counts per week are contributing way more than any other feature, with some evnts per session also contributing a fair amount. Account age was less predicting than I had hoped, and gender doesn't seem to contribute at all which makes intuitive sense.\n\nAll in all, the tuned random forest is the best model that we have with a f1-score of 0.79.\n\nThe refinement section with random forests gave me a ton of trouble, it would routinely crash my EMR notebook. This was by far the hardest part of the entire project for me and probably ate up over 6-8 hours of my time. In the end, the problem was that the 'Spark Job Progress' box would take too much memory and chrome would kill the tab. If you want to run the cells be sure to **CLEAR THE CELL OUTPUT WHILE IT IS RUNNING**.\n\nOther than that, the hardest part of the project was probably feature engineering using spark. Everything takes quite a bit longer than with local datasets! Be prepared to wait."}, {"metadata": {}, "cell_type": "markdown", "source": "## Conclusion"}, {"metadata": {}, "cell_type": "markdown", "source": "To sum it up, we found and engineered a useful set of features, tried a few different models, and ended up with a fairly well performing model with an f1-score of 0.79. A few alternatives were tried, event though we weren't going for the highest performance, more for a proof of concept. The random forest ended up being the highest performing model, which isn't too surprising compared to decision trees and logistic regression.\n\nIt's also important to consider the robustness of the model. We gain some insight into this from using a 3 Fold cross validator and looking at the average scores of the different models. We saw earlier that even with varied parameters each model had around the same score, which indicates that the model is pretty robust to deviations in the training data.\n\nAlso, the type of data that is entered into this model is unlikely to have large scale changes unless users actually change their behavior. Events data lends itself to features that are abstracted from the density and sequence of events, and small changes in the number of events are unlikely to cause big swings in any particular feature. We even saw that the most impactful features to distinguish churned from non-churned users was the number of songs they listened to in a given week.\n\nThis method has proven to be pretty effective, and can clearly distinguish between churned and non churned users when provided the entire timeframe of data. This could be useful in a production service to provide some sort of promotion or treatment to users that are identified as likely to churn.\n\nI think both the hardest part and the most interesting part of this project was identifying and extracting features from the events data. It's much less intuitive than many projects we had in the class, so it provided a nice challenge. I would say my main goal for this project was to learn the basics of PySpark, and I think I accomplished that nicely. I have a few other interesting ideas that I didn't have time to investigate in this project."}, {"metadata": {}, "cell_type": "markdown", "source": "## Improvements"}, {"metadata": {}, "cell_type": "markdown", "source": "In this project I found a lot of interesting similarites to the recommendation engines section of the nanodegree even though it wasn't implemented in the same way. A really interesting improvement to this project could be to approach the problem as a timerange problem rather than a random split problem - training on a few weeks of data and testing to see if you can predict the users who churn in the following few weeks. This would be a much more realistic way to apply the model to how we'd like to use it in the real world. It does present a bunch of additional challenges and might require some custom algorithms/evaluators which makes it a more longer term project.\n\nAnother obvious extension would be to look for instances of `Submit Downgrade` instead of the cancellation events. This would be tracking users who are going to downgrade from paid to free. It would require a different set of features, likely more difficult ones since it would be required to separate different user periods by level. It also would benefit to a similar approach mentioned above, combining the two would be very interesting.\n\nThat about wraps my project, thanks for reading!"}], "metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": ""}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 2}, "pygments_lexer": "python2"}}, "nbformat": 4, "nbformat_minor": 2}